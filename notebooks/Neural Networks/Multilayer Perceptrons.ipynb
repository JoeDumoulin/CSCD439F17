{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    "\n",
    "You can read a good introduction to Multilayer Perceptrons (MLPs) at [scikit-learns page](http://scikit-learn.org/stable/modules/neural_networks_supervised.html).\n",
    "\n",
    "MLPs are used to solve the same problems as regressions, but they make it possible to solve for non-linear relationships and they allow us to do that without defining feature details (i.e, without having to have expert knowledge of the domain).\n",
    "\n",
    "A MLP can predict non-linear relationships in data by separating the input and output data layers by one or more intermediate (\"hidden\") layers.  Hidden layers accumulate interemediate values of the relation and then provide them as input for the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer is a set of nodes.  The number of nodes in the layer is set depending on the problem solved by the network.  Below I am including a picture from the scikit-learn documentation linked above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://scikit-learn.org/stable/_images/multilayerperceptron_network.png\" width=50%>A Multilayer Perceptron</img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input layer** represents the features present in each input to the network.  For example, if the network is designed to recognize images, the network will usually require the input layer to accept individual pixels from an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MLP includes one or more **hidden layer** which is used to separate calculations and generate intermediate features used in the network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the MLP will have a set of nodes that form the **output layer**.  The output may provide a regression output, but that is unusual.  Most of the time a MLP will be used to predict a class of responses.  In this case, each output node will represent a specific class.  For example, if the network was built to recognize digits, then there would be 10 nodes in the output, one for each digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between each layer of nodes is a set of **weights**, each of which connects a single node in one layer to a node in the next layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNIST Logistic Regression\n",
    "import idx2numpy\n",
    "\n",
    "X = idx2numpy.convert_from_file('train-images-idx3-ubyte')\n",
    "y = idx2numpy.convert_from_file('train-labels-idx1-ubyte')\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce dimensions on X.  Keep first dim but comine second and third\n",
    "X = X.reshape(X.shape[0],-1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs').fit(X, y)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = idx2numpy.convert_from_file('t10k-images-idx3-ubyte')\n",
    "y_test = idx2numpy.convert_from_file('t10k-labels-idx1-ubyte')\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce dimensions on X.  Keep first dim but comine second and third\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 957    0    0    4    0    3    6    2    6    2]\n",
      " [   0 1116    3    1    0    1    4    1    8    1]\n",
      " [   8   12  906   18    9    5   10   11   50    3]\n",
      " [   3    0   19  916    2   23    5   11   24    7]\n",
      " [   1    2    5    3  910    0   11    2   10   38]\n",
      " [  11    2    1   40   10  754   16    8   39   11]\n",
      " [   7    3    7    2    4   17  909    1    8    0]\n",
      " [   3    6   24    4    7    1    1  946    5   31]\n",
      " [   9   15    7   22   11   26    7   12  854   11]\n",
      " [   9    6    2   13   30    4    0   25   16  904]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "\n",
    "# print confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_predicted)\n",
    "print('Confusion matrix')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       980\n",
      "          1       0.96      0.98      0.97      1135\n",
      "          2       0.93      0.88      0.90      1032\n",
      "          3       0.90      0.91      0.90      1010\n",
      "          4       0.93      0.93      0.93       982\n",
      "          5       0.90      0.85      0.87       892\n",
      "          6       0.94      0.95      0.94       958\n",
      "          7       0.93      0.92      0.92      1028\n",
      "          8       0.84      0.88      0.86       974\n",
      "          9       0.90      0.90      0.90      1009\n",
      "\n",
      "avg / total       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 945    1    6    1    0    5   10    6    6    0]\n",
      " [   0 1109    3    3    0    1    2    1   15    1]\n",
      " [   4    3  977   16    5    2    6   10    6    3]\n",
      " [   1    2   13  943    0   20    1    3   19    8]\n",
      " [   0    1   10    1  923    3    6    2    6   30]\n",
      " [   5    1    0   24    1  822   13    2   18    6]\n",
      " [   7    4    5    0    3    9  922    2    6    0]\n",
      " [   2    4   20    7    5    4    0  955    5   26]\n",
      " [   5    0    6    9    5   10    4    4  922    9]\n",
      " [   3    5    4    7   29    7    0    7   14  933]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97       980\n",
      "          1       0.98      0.98      0.98      1135\n",
      "          2       0.94      0.95      0.94      1032\n",
      "          3       0.93      0.93      0.93      1010\n",
      "          4       0.95      0.94      0.95       982\n",
      "          5       0.93      0.92      0.93       892\n",
      "          6       0.96      0.96      0.96       958\n",
      "          7       0.96      0.93      0.95      1028\n",
      "          8       0.91      0.95      0.93       974\n",
      "          9       0.92      0.92      0.92      1009\n",
      "\n",
      "avg / total       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now with MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(100), random_state=1)\n",
    "mlp.fit(X,y)\n",
    "\n",
    "y_predicted = mlp.predict(X_test)\n",
    "\n",
    "\n",
    "# print confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_predicted)\n",
    "print('Confusion matrix')\n",
    "print(confusion)\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.92      0.94       980\n",
      "          1       0.99      0.97      0.98      1135\n",
      "          2       0.81      0.89      0.85      1032\n",
      "          3       0.86      0.81      0.84      1010\n",
      "          4       0.93      0.78      0.85       982\n",
      "          5       0.84      0.75      0.79       892\n",
      "          6       0.93      0.93      0.93       958\n",
      "          7       0.95      0.88      0.91      1028\n",
      "          8       0.76      0.87      0.81       974\n",
      "          9       0.77      0.92      0.84      1009\n",
      "\n",
      "avg / total       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#another model with different sized hidden layer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp2 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(50), random_state=1)\n",
    "mlp2.fit(X,y)\n",
    "\n",
    "y_predicted = mlp2.predict(X_test)\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98       980\n",
      "          1       0.99      0.98      0.99      1135\n",
      "          2       0.96      0.96      0.96      1032\n",
      "          3       0.94      0.96      0.95      1010\n",
      "          4       0.97      0.96      0.97       982\n",
      "          5       0.95      0.95      0.95       892\n",
      "          6       0.97      0.97      0.97       958\n",
      "          7       0.96      0.97      0.96      1028\n",
      "          8       0.95      0.94      0.95       974\n",
      "          9       0.96      0.95      0.96      1009\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#one more model with different sized hidden layer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp3 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(200), random_state=1)\n",
    "mlp3.fit(X,y)\n",
    "y_predicted = mlp3.predict(X_test)\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.97      0.96      0.97      1032\n",
      "          3       0.96      0.97      0.96      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.96      0.95      0.95       892\n",
      "          6       0.97      0.97      0.97       958\n",
      "          7       0.97      0.96      0.97      1028\n",
      "          8       0.96      0.95      0.96       974\n",
      "          9       0.95      0.96      0.96      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#one more model with different sized hidden layer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp4 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(400), random_state=1)\n",
    "mlp4.fit(X,y)\n",
    "y_predicted = mlp4.predict(X_test)\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.96      0.96      0.96      1032\n",
      "          3       0.96      0.96      0.96      1010\n",
      "          4       0.97      0.96      0.96       982\n",
      "          5       0.95      0.96      0.96       892\n",
      "          6       0.98      0.97      0.97       958\n",
      "          7       0.97      0.97      0.97      1028\n",
      "          8       0.96      0.96      0.96       974\n",
      "          9       0.95      0.94      0.95      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now a fully connected deep model.\n",
    "#one more model with different sized hidden layer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpdeep = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(100, 50), random_state=1)\n",
    "mlpdeep.fit(X,y)\n",
    "y_predicted = mlpdeep.predict(X_test)\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 963    0    3    0    1    5    1    5    2    0]\n",
      " [   0 1122    5    1    0    1    1    0    4    1]\n",
      " [   7    1  989   13    3    2    3    7    7    0]\n",
      " [   0    3   10  972    1    9    0    3    9    3]\n",
      " [   1    0    5    0  943    1    9    1    2   20]\n",
      " [   2    0    1   12    0  859    5    0    6    7]\n",
      " [   8    2    0    0    7    6  931    0    4    0]\n",
      " [   3    3   10    1    0    0    0  993    5   13]\n",
      " [   4    1    7    5    3    8    2    5  937    2]\n",
      " [   2    3    0    9   17   11    1   11    4  951]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
