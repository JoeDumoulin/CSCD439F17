{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Logistic Regression is a simple method for predicting classifications for data.  Simple regression will perform binary classification (for example, diagnosing whether or not cancer is present).  Multinomial logistic regression can learn to predict multiple classes from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is very similar to linear regression.  The difference is in the details of the hypothesis function $h(x)$ and the cost function $J(\\theta)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With linear regression, $h(x)=\\theta^Tx$ and $J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m(y-h)^2$.  In this case, $h(x)$ predicts directly the value of $y$.  $h$ and $J$ are different with logistic regression.  $h(x)$ predicts the probability of the row being in one category or other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $h(x)$\n",
    "Let's look at logistic regression where we are trying to predict whether a response is in a class or not.  Then we want $h(x)$ to represent the probability that the response is in the class.  For example, suppose we were trying to diagnose the presence of cancer based on some tests.  Then $h(x) = P(y=1 \\ |\\  x;\\theta)$, $h$ is the probability that $y$ is $1$ given the test results $x$ and the regression parameters $\\theta$.  What does $h$ look like in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the expression above, we also know an expression for $P(y=0 \\ |\\ x;\\theta) = 1-h(x)$.  Then there are a couple of identities that we will know about $h(x)$ and $P$:\n",
    "\n",
    "$$ P(y=1) + P(y=0) = 1$$\n",
    "and \n",
    "$$ \\frac{P(y=1)}{P(y=0)} = \\frac{h(x)}{1-h(x)} $$ \n",
    "\n",
    "Which implies that when $P(y=1) = P(y=0)$, \n",
    "\n",
    "$$ h(x) = 1 - h(x) \\implies h(x) = 0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of functions that fit these features, but logistic regression is named after a particular function called the **logit function**: \n",
    "\n",
    "$$ g(z) = \\frac{1}{1-\\exp^{-z}}$$\n",
    "\n",
    "if we use the logit to find the probability of class membership, then:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\frac{1}{1-\\exp^{-\\theta^Tx}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit gives us the means to meet the requirements we set out for $h(x)$ and we can develop a useful cost function for learning from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Cost Function $J(\\theta)$\n",
    "\n",
    "The cost function expresses the error over the training data during the training function.  Machine learning, in general, can be thought of as an optimization problem that serves to minimize the cost function.\n",
    "\n",
    "Since $h(x)$ represents a probability, $P(y=1)$, what we would like is for \n",
    "\n",
    " * $Cost = 0$ when $h(x) = 1$ and $y \\in 1$, \n",
    " * $Cost \\Rightarrow \\infty$ when $h(x) = 0$ and $y \\in 1$,\n",
    " * $Cost = 0$ when $h(x) = 0$ and $y \\in 0$, and\n",
    " * $Cost \\Rightarrow \\infty$ when $h(x) = 1$ and $y \\in 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One easy way to support these relationships is to use natural logarithm functions to describe the cost function.  In this case we have:\n",
    "\n",
    "* $Cost(h_{\\theta}(x), y=1) = -log(h_{\\theta}(x))$\n",
    "* $Cost(h_{\\theta}(x), y=0) = -log(1 - h_{\\theta}(x))$\n",
    "\n",
    "Where $0$ and $1$ are the values that $y$ can take on (the 'classes' recognized by the classifier after training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start looking at what $J(\\theta)$ looks like for logistic regression.  In general, $J(\\theta)$ is a *total* cost function used for two purposes:\n",
    "\n",
    "* To direct the optimization central to training machine learning systems (Training), \n",
    "* To check the accuracy of the resulting system against test data (Testing).\n",
    "\n",
    "So, in general, $J(\\theta)$ is the sum of the cost of each row of training data.  The training data is divided into the predictor data $x^{(i)}$ and the *actual* response data $y(i)$ for each row $i$.  The sum is then:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m Cost(h_{\\theta}(x^{(i)}), y^{(i)})$$\n",
    "\n",
    "This is a general expression for $J(\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plug in our cost function for logistic regression to get the expression we will optimize.  Currently our expression for cost is broken into two parts, one for $y=0$ and another for $y=1$.  We would prefer to optimize a single expression so we combine the two as follows:\n",
    "\n",
    "$$ Cost(h_{\\theta}(x), y) = -(y\\ log(h_{\\theta}(x)) + (1 - y)log(1 - h_{\\theta}(x)))$$.\n",
    "\n",
    "With this expression for cost, we can now write out the expression for $J(\\theta)$ as:\n",
    "\n",
    "$$J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^m (y^{(i)} log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})log(1 - h_{\\theta}(x^{(i)})))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Update Rule for Learning $\\theta$\n",
    "The general update rule provides us with a way to iterate, improving our estimate for $\\theta$ at each step.  The rule use gradient descent, so generally, \n",
    "\n",
    "$$ \\theta_j^{t+1} = \\theta_j^t - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j^t} $$\n",
    "\n",
    "Where $j$ is the 'dimension' (or column) of the $x_j$ variable associated with $\\theta_j$, $t$ is the time step of the iteration, and $\\alpha$ is the learning rate.  Again, this is a general expression, true of any machine learning method's update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this rule specific for logistic regression, we get the following update rule:\n",
    "\n",
    "$$ \\theta_j^{t+1} = \\theta_j^t - \\alpha \\sum_{i=1}^m ((h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}) $$\n",
    "\n",
    "Where $h_{\\theta}(x)$ is defined as above.  Note that this rule works for any number of predictors in the logistic regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
