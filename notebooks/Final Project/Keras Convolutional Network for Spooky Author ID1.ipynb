{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebok, we addapt the text categorization problem outlined in the [keras tutorial documentation](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).  We begin by simply changing the data source to be the input data to the data from the kaggle contest [Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification/kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6727931944819527565\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10904823399\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 368414887088364895\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "# Definitions\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow settings to activate gpu\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "BASE_DIR = '../data'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'SpookyData')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "import tensorflow as tf\n",
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the data into a form where we can use it for training and prediction.  For that we use the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the training data\n",
    "df = pd.read_csv(os.path.join(TEXT_DATA_DIR, 'train.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EAP': 0, 'MWS': 2, 'HPL': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   author_id  \n",
       "0          0  \n",
       "1          1  \n",
       "2          0  \n",
       "3          2  \n",
       "4          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of classifications and generate numeric \n",
    "#  values for each class.  put the numeric class back \n",
    "#  on to the data frame.\n",
    "authors = dict([(auth, idx) for idx, auth in enumerate(df['author'].unique())])\n",
    "print(authors)\n",
    "df['author_id'] = df['author'].apply(lambda x: authors[x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "[26, 2945, 143, 1372, 22, 36, 294, 2, 7451, 1, 2440, 2, 10, 4556, 16, 6, 79, 179, 48, 4245, 3, 295, 4, 1, 249, 1943, 6, 326, 74, 134, 123, 891, 2, 1, 313, 39, 1438, 4928, 98, 1, 430]\n",
      "Found 25943 unique tokens before stopwords removal.\n",
      "[('superhuman', 7725), ('ripples', 7813), ('singleness', 12350), ('dig', 8374), ('reasons', 2213)]\n",
      "Found 25808 unique tokens after stopwords removal.\n",
      "Shape of data tensor: (19579, 1000)\n",
      "Shape of label tensor: (19579, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop stop words\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# now we will use the text and author_id fields to train a classifier.\n",
    "#  We have to: \n",
    "#  1. Get the sentences, \n",
    "sents = df['text'].tolist()\n",
    "labels = df['author_id'].tolist()\n",
    "#  2. Tokenize each sentence, \n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sents)\n",
    "sequences = tokenizer.texts_to_sequences(sents)\n",
    "print(len(sequences))\n",
    "print(sequences[0])\n",
    "##    Get a vector of unique terms here\n",
    "print('Found %s unique tokens before stopwords removal.' % len(tokenizer.word_index))\n",
    "print([w for w in tokenizer.word_index.items()][:5])\n",
    "word_index = dict([(w,i) for w,i in tokenizer.word_index.items() if w not in stops])\n",
    "print('Found %s unique tokens after stopwords removal.' % len(word_index))\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "y_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#  3. Load embeddings\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092\n"
     ]
    }
   ],
   "source": [
    "#  4. Create the Embedding matrix for the training set\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "unk = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk.append(word)\n",
    "print(len(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "#x = MaxPooling1D()(x)\n",
    "#x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "preds = Dense(len(authors), activation='softmax')(x)\n",
    "rms = RMSprop(lr=0.003)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='mean_squared_logarithmic_error',\n",
    "              optimizer=rms, #'rmsprop',\n",
    "              metrics=['acc'])\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=rms, #'rmsprop',\n",
    "#              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/50\n",
      "15664/15664 [==============================] - 6s 391us/step - loss: 0.0763 - acc: 0.6522 - val_loss: 0.0537 - val_acc: 0.7665\n",
      "Epoch 2/50\n",
      "15664/15664 [==============================] - 5s 322us/step - loss: 0.0381 - acc: 0.8409 - val_loss: 0.0553 - val_acc: 0.7709\n",
      "Epoch 3/50\n",
      "15664/15664 [==============================] - 5s 329us/step - loss: 0.0204 - acc: 0.9182 - val_loss: 0.0561 - val_acc: 0.7803\n",
      "Epoch 4/50\n",
      "15664/15664 [==============================] - 5s 330us/step - loss: 0.0124 - acc: 0.9515 - val_loss: 0.0515 - val_acc: 0.8061\n",
      "Epoch 5/50\n",
      "15664/15664 [==============================] - 5s 330us/step - loss: 0.0083 - acc: 0.9672 - val_loss: 0.0513 - val_acc: 0.8102\n",
      "Epoch 6/50\n",
      "15664/15664 [==============================] - 5s 327us/step - loss: 0.0060 - acc: 0.9776 - val_loss: 0.0633 - val_acc: 0.7816\n",
      "Epoch 7/50\n",
      "15664/15664 [==============================] - 5s 331us/step - loss: 0.0048 - acc: 0.9820 - val_loss: 0.0560 - val_acc: 0.8064\n",
      "Epoch 8/50\n",
      "15664/15664 [==============================] - 5s 333us/step - loss: 0.0045 - acc: 0.9830 - val_loss: 0.0597 - val_acc: 0.7980\n",
      "Epoch 9/50\n",
      "15664/15664 [==============================] - 5s 334us/step - loss: 0.0035 - acc: 0.9876 - val_loss: 0.0586 - val_acc: 0.8008\n",
      "Epoch 10/50\n",
      "15664/15664 [==============================] - 5s 329us/step - loss: 0.0037 - acc: 0.9871 - val_loss: 0.0588 - val_acc: 0.8026\n",
      "Epoch 11/50\n",
      "15664/15664 [==============================] - 5s 334us/step - loss: 0.0033 - acc: 0.9883 - val_loss: 0.0596 - val_acc: 0.8031\n",
      "Epoch 12/50\n",
      "15664/15664 [==============================] - 5s 329us/step - loss: 0.0034 - acc: 0.9880 - val_loss: 0.0582 - val_acc: 0.8074\n",
      "Epoch 13/50\n",
      "15664/15664 [==============================] - 5s 335us/step - loss: 0.0031 - acc: 0.9891 - val_loss: 0.0586 - val_acc: 0.8072\n",
      "Epoch 14/50\n",
      "15664/15664 [==============================] - 5s 335us/step - loss: 0.0035 - acc: 0.9879 - val_loss: 0.0642 - val_acc: 0.7895\n",
      "Epoch 15/50\n",
      "15664/15664 [==============================] - 5s 331us/step - loss: 0.0027 - acc: 0.9907 - val_loss: 0.0592 - val_acc: 0.8072\n",
      "Epoch 16/50\n",
      "15664/15664 [==============================] - 5s 332us/step - loss: 0.0031 - acc: 0.9894 - val_loss: 0.0592 - val_acc: 0.8069\n",
      "Epoch 17/50\n",
      "15664/15664 [==============================] - 5s 335us/step - loss: 0.0031 - acc: 0.9893 - val_loss: 0.0585 - val_acc: 0.8105\n",
      "Epoch 18/50\n",
      "15664/15664 [==============================] - 5s 334us/step - loss: 0.0024 - acc: 0.9917 - val_loss: 0.0605 - val_acc: 0.8028\n",
      "Epoch 19/50\n",
      "15664/15664 [==============================] - 5s 329us/step - loss: 0.0025 - acc: 0.9917 - val_loss: 0.0597 - val_acc: 0.8046\n",
      "Epoch 20/50\n",
      "15664/15664 [==============================] - 5s 335us/step - loss: 0.0028 - acc: 0.9906 - val_loss: 0.0589 - val_acc: 0.8097\n",
      "Epoch 21/50\n",
      "15664/15664 [==============================] - 5s 332us/step - loss: 0.0026 - acc: 0.9913 - val_loss: 0.0576 - val_acc: 0.8138\n",
      "Epoch 22/50\n",
      "15664/15664 [==============================] - 5s 337us/step - loss: 0.0025 - acc: 0.9916 - val_loss: 0.0585 - val_acc: 0.8128\n",
      "Epoch 23/50\n",
      "15664/15664 [==============================] - 5s 334us/step - loss: 0.0023 - acc: 0.9921 - val_loss: 0.0611 - val_acc: 0.8023\n",
      "Epoch 24/50\n",
      "15664/15664 [==============================] - 5s 333us/step - loss: 0.0022 - acc: 0.9927 - val_loss: 0.0564 - val_acc: 0.8186\n",
      "Epoch 25/50\n",
      "15664/15664 [==============================] - 5s 337us/step - loss: 0.0022 - acc: 0.9926 - val_loss: 0.0601 - val_acc: 0.8072\n",
      "Epoch 26/50\n",
      "15664/15664 [==============================] - 5s 329us/step - loss: 0.0027 - acc: 0.9908 - val_loss: 0.0589 - val_acc: 0.8100\n",
      "Epoch 27/50\n",
      "15664/15664 [==============================] - 5s 335us/step - loss: 0.0027 - acc: 0.9910 - val_loss: 0.0600 - val_acc: 0.8079\n",
      "Epoch 28/50\n",
      "15664/15664 [==============================] - 5s 339us/step - loss: 0.0021 - acc: 0.9929 - val_loss: 0.0588 - val_acc: 0.8112\n",
      "Epoch 29/50\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.0024 - acc: 0.9923 - val_loss: 0.0578 - val_acc: 0.8140\n",
      "Epoch 30/50\n",
      "15664/15664 [==============================] - 5s 335us/step - loss: 0.0026 - acc: 0.9914 - val_loss: 0.0609 - val_acc: 0.8051\n",
      "Epoch 31/50\n",
      "15664/15664 [==============================] - 5s 331us/step - loss: 0.0021 - acc: 0.9930 - val_loss: 0.0609 - val_acc: 0.8054\n",
      "Epoch 32/50\n",
      "15664/15664 [==============================] - 5s 333us/step - loss: 0.0024 - acc: 0.9920 - val_loss: 0.0581 - val_acc: 0.8130\n",
      "Epoch 33/50\n",
      "15664/15664 [==============================] - 5s 333us/step - loss: 0.0020 - acc: 0.9933 - val_loss: 0.0577 - val_acc: 0.8143\n",
      "Epoch 34/50\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.0018 - acc: 0.9941 - val_loss: 0.0598 - val_acc: 0.8072\n",
      "Epoch 35/50\n",
      "15664/15664 [==============================] - 5s 337us/step - loss: 0.0022 - acc: 0.9927 - val_loss: 0.0624 - val_acc: 0.8013\n",
      "Epoch 36/50\n",
      "15664/15664 [==============================] - 5s 339us/step - loss: 0.0019 - acc: 0.9937 - val_loss: 0.0589 - val_acc: 0.8117\n",
      "Epoch 37/50\n",
      "15664/15664 [==============================] - 5s 334us/step - loss: 0.0020 - acc: 0.9934 - val_loss: 0.0590 - val_acc: 0.8102\n",
      "Epoch 38/50\n",
      "15664/15664 [==============================] - 5s 344us/step - loss: 0.0019 - acc: 0.9938 - val_loss: 0.0593 - val_acc: 0.8092\n",
      "Epoch 39/50\n",
      "15664/15664 [==============================] - 5s 339us/step - loss: 0.0021 - acc: 0.9933 - val_loss: 0.0590 - val_acc: 0.8117\n",
      "Epoch 40/50\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.0022 - acc: 0.9931 - val_loss: 0.0604 - val_acc: 0.8066\n",
      "Epoch 41/50\n",
      "15664/15664 [==============================] - 5s 333us/step - loss: 0.0020 - acc: 0.9934 - val_loss: 0.0600 - val_acc: 0.8092\n",
      "Epoch 42/50\n",
      "15664/15664 [==============================] - 5s 338us/step - loss: 0.0019 - acc: 0.9938 - val_loss: 0.0594 - val_acc: 0.8095\n",
      "Epoch 43/50\n",
      "15664/15664 [==============================] - 5s 330us/step - loss: 0.0021 - acc: 0.9929 - val_loss: 0.0585 - val_acc: 0.8138\n",
      "Epoch 44/50\n",
      "15664/15664 [==============================] - 5s 350us/step - loss: 0.0018 - acc: 0.9941 - val_loss: 0.0573 - val_acc: 0.8161\n",
      "Epoch 45/50\n",
      "15664/15664 [==============================] - 5s 339us/step - loss: 0.0018 - acc: 0.9943 - val_loss: 0.0586 - val_acc: 0.8115\n",
      "Epoch 46/50\n",
      "15664/15664 [==============================] - 5s 334us/step - loss: 0.0018 - acc: 0.9939 - val_loss: 0.0590 - val_acc: 0.8117\n",
      "Epoch 47/50\n",
      "15664/15664 [==============================] - 5s 344us/step - loss: 0.0019 - acc: 0.9939 - val_loss: 0.0567 - val_acc: 0.8171\n",
      "Epoch 48/50\n",
      "15664/15664 [==============================] - 5s 328us/step - loss: 0.0017 - acc: 0.9946 - val_loss: 0.0580 - val_acc: 0.8148\n",
      "Epoch 49/50\n",
      "15664/15664 [==============================] - 5s 330us/step - loss: 0.0022 - acc: 0.9930 - val_loss: 0.0612 - val_acc: 0.8059\n",
      "Epoch 50/50\n",
      "15664/15664 [==============================] - 5s 344us/step - loss: 0.0018 - acc: 0.9943 - val_loss: 0.0573 - val_acc: 0.8179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c954ff8d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=50,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 6,209,027\n",
      "Trainable params: 6,209,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
