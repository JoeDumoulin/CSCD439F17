{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Text processing has a long history and is known under a couple of different names including Computational Linguistics and Natural Language Processing.  in this notebook we will look at some text processing options avaiable in the packages\n",
    "\n",
    "* nltk\n",
    "* gensim\n",
    "* scikit-learn\n",
    "\n",
    "### nltk - The Natural Language Toolkit\n",
    "nltk is a very popular text processing tool for western languages.  It can be found at the [nltk site](http://www.nltk.org/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/joe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/joe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "(19579, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# load training data\n",
    "BASE_DIR = '../data'\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'SpookyData')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# read the training data\n",
    "df = pd.read_csv(os.path.join(TEXT_DATA_DIR, 'train.csv'))\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n"
     ]
    }
   ],
   "source": [
    "# Read the text of the training examples  \n",
    "corpus = df['text'].tolist()\n",
    "print(corpus[0])\n",
    "unique_labels = df['author'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'process', ',', 'however', ',', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', ';', 'as', 'I', 'might', 'make', 'its', 'circuit', ',', 'and', 'return', 'to', 'the', 'point', 'whence', 'I', 'set', 'out', ',', 'without', 'being', 'aware', 'of', 'the', 'fact', ';', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'process', ',', 'however', ',', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', ';', 'I', 'might', 'make', 'circuit', ',', 'return', 'point', 'whence', 'I', 'set', ',', 'without', 'aware', 'fact', ';', 'perfectly', 'uniform', 'seemed', 'wall', '.']\n",
      "['EAP', 'HPL', 'MWS']\n",
      "(['This', 'process', ',', 'however', ',', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', ';', 'I', 'might', 'make', 'circuit', ',', 'return', 'point', 'whence', 'I', 'set', ',', 'without', 'aware', 'fact', ';', 'perfectly', 'uniform', 'seemed', 'wall', '.'], 'EAP')\n"
     ]
    }
   ],
   "source": [
    "# Remove english stopwords from the tokenized lists\n",
    "stops = set(stopwords.words('english'))\n",
    "modified_corpus = []\n",
    "for sent in corpus:\n",
    "    modified_sent = []\n",
    "    for term in word_tokenize(sent):\n",
    "        if term not in stops:\n",
    "            modified_sent.append(term)\n",
    "    modified_corpus.append(modified_sent)\n",
    "print(modified_corpus[0])\n",
    "\n",
    "labels = df['author'].tolist()\n",
    "print(unique_labels)\n",
    "\n",
    "labeled_corpus = list(zip(modified_corpus, labels))\n",
    "print(labeled_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'however': True, 'means': True, ';': True, 'ascertaining': True, 'set': True, 'might': True, 'afforded': True, 'fact': True, 'wall': True, 'make': True, 'without': True, 'dungeon': True, ',': True, '.': True, 'return': True, 'circuit': True, 'dimensions': True, 'perfectly': True, 'seemed': True, 'uniform': True, 'This': True, 'process': True, 'point': True, 'whence': True, 'aware': True, 'I': True}, 'EAP')\n",
      "['however', 'means', ';', 'ascertaining', 'set', 'might', 'afforded', 'fact', 'wall', 'make', 'without', 'dungeon', ',', '.', 'return', 'circuit', 'dimensions', 'perfectly', 'seemed', 'uniform', 'This', 'process', 'point', 'whence', 'aware', 'I']\n"
     ]
    }
   ],
   "source": [
    "# create a labeled set of training features.\n",
    "len(all_words)\n",
    "# another way to say the same thing...\n",
    "#all_words = set(word.lower() for passage in modified_corpus for word in passage[0])\n",
    "all_data = []\n",
    "for passage in labeled_corpus:\n",
    "    d = {}\n",
    "    for term in passage[0]:\n",
    "        d[term] = True\n",
    "    all_data.append((d, passage[1]))\n",
    "    \n",
    "print(all_data[0])\n",
    "print(list(all_data[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['refinery', 'humankind', 'epilepsy', 'execution', 'toothed']\n",
      "[18749, 441, 12128, 5028, 25836]\n",
      "['stores', 'wherewith', 'admonishing', 'darkening', 'abyss']\n"
     ]
    }
   ],
   "source": [
    "#get a random set of features to use to create negative examples for training.\n",
    "all_words = set()               # uniques terms\n",
    "for passage in labeled_corpus:\n",
    "    for word in passage[0]:\n",
    "        all_words.add(word)\n",
    "\n",
    "all_words = list(all_words)\n",
    "print(all_words[:5])\n",
    "\n",
    "all_words_idx = [i for i,_ in enumerate(all_words)]\n",
    "\n",
    "# shuffle the indexes so that we can produce reamdom samples\n",
    "from random import shuffle\n",
    "\n",
    "shuffled_word_idxs = [i for i,_ in enumerate(all_words)]\n",
    "shuffle(shuffled_word_idxs)\n",
    "print(shuffled_word_idxs[:5])\n",
    "print([all_words[shuffled_idx] for shuffled_idx in shuffled_word_idxs[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Ellery': False, 'might': True, 'transmission': False, 'individuallity': False, 'Sailing': False, 'cranium': False, 'seasons': False, 'outreaching': False, 'stood': False, 'logical': False, 'Puritan': False, 'alienists': False, 'sociable': False, 'Being': False, 'flaunts': False, 'pangs': False, 'hunting': False, 'emerged': False, 'caves': False, 'adopting': False, 'helmet': False, 'profile': False, 'waddle': False, 'touches': False, 'Thurai': False, 'Crebillon': False, 'spiced': False, 'memory': False, 'mistake': True, 'insulted': False, 'villanous': False, 'tenets': False, 'reacted': False, 'millionaire': False, 'fractionally': False, 'brightly': False, 'letting': False, 'stamped': False, 'inurned': False, 'mechanistic': False, 'two': False, 'wrapping': False, 'youth': False, 'portcullis': False, 'flagrant': False, 'gratified': False, 'assertion': False, 'brain': False, 'Lor': False, 'Mister': False, 'Norfolk': False, 'interwoven': False, 'Quizzem': False, 'brightest': False, '.': True, 'warning': False, 'rigorously': False, 'Lettres': False, 'finny': False, 'unmeaning': False, 'kicking': False, 'Savage': False, 'propagated': False, 'allured': False, 'fumbling': True, 'conceivable': False, 'Bacchanals': False, 'evinces': False, 'patter': False, 'impresses': False, 'waited': False, 'cupidity': False, 'Harvard': False, 'tartan': False, 'older': False, 'fatigue': False, 'promptings': False, 'heerd': False, 'getting': False, 'listening': False, 'inviolable': False, 'Corners': False, 'minced': False, 'never': True, 'roseal': False, 'mere': True, 'handsomely': False, 'fuzzy': False, 'occurred': True, 'uninscribable': False, 'parchment': False, 'Talks': False, 'renamed': False, 'streak': False, 'Rowley': False, 'cognizance': False, 'Wisconsin': False, 'incorporated': False, 'sociologist': False, 'eater': False, 'vanities': False, 'haouses': False, 'Principia': False, 'boon': False, 'hev': False, 'victory': False, 'dissolved': False, 'labyrinths': False, 'It': True, 'mix': False, 'retouch': False, 'outspeed': False, 'unfolded': False, 'masse': False, 'GALLOWS': False, 'debility': False, 'skiff': False, 'intolerant': False, 'ungenteel': False, 'unfrequented': False, 'authentick': False, 'gravely': False, 'avidity': False, 'workmanship': False, 'cooing': False, 'liquors': False, 'clues': False, 'Devoted': False}, 'HPL')\n"
     ]
    }
   ],
   "source": [
    "# Add some of the shuffled terms as negative examples for each of the data samples.\n",
    "allw = len(all_words)\n",
    "idx = 0 # we are going to loop trough the shuffled values.\n",
    "for passage in all_data:\n",
    "    sample = list(passage[0].keys())\n",
    "    j = 0\n",
    "    #print(sample) \n",
    "    while j < len(sample): #  add the same number of negative samples as positive.\n",
    "        current = all_words[shuffled_word_idxs[idx]]\n",
    "        #print(current)\n",
    "        if current not in sample:\n",
    "            #  add the current term as a negative sample\n",
    "            passage[0][current] = False\n",
    "            ## increment j\n",
    "            j = j+1\n",
    "        ## increment index counter\n",
    "        idx = idx+1\n",
    "        if idx == allw:\n",
    "            idx = 0 # reset and go around again\n",
    "                \n",
    "print(all_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sagacity': False, 'toiled': False, 'gait': False, 'advancing': True, 'endures': False, 'Whisper': False, 'veneration': False, 'technical': False, 'resumed': True, 'stragglers': True, 'paralyzed': False, 'despotic': False, 'bran': False, 'progress': True, 'Chantilly': False, \"'singular\": False, 'slopes': False, 'thoroughfare': True, 'hurriedly': True, 'Organ': False, 'handkerchiefs': False, 'inspect': False, 'around': True, 'advised': False, 'got': False, 'harmonies': False, 'band': True, 'I': True, 'Eliot': True, 'attain': False, 'conqueror': False, 'satisfactorily': False, 'mine': False, 'Street': True, 'Bleu': False, 'incurred': False, 'kindred': False, 'stems': False, 'impose': False, 'still': True, 'sheltering': False, 'secured': False, 'felled': False, 'rove': False, 'emigrant': False, 'entombment': False, 'thanks': False, 'Adonis': False, 'darting': True, ';': True, 'When': True, 'corner': True, 'handling': False, 'along': True, 'precise': False, 'impossibilities': False, 'Flag': False, 'medals': False, 'Foie': False, 'afur': False, 'aggregated': False, 'Fuseli': False, 'crossing': True, 'slayer': False, 'chanced': False, 'studding': False, 'Marquis': False, 'lest': True, 'Turkish': False, '.': True, 'Monument': False, 'unbeheld': False, 'demons': False, 'Poisoned': False, 'fifty': False, 'enwrapped': False, 'enquiry': False, 'reserving': False, 'preconcert': False, 'spark': False, 'commands': False, 'last': True, 'handfuls': False, 'authenticated': False, 'admired': False, 'transition': False, 'insidious': False, 'sight': True, 'Did': False, 'party': True, ',': True, 'Lafayette': True, 'meditations': False, 'helpless': False, 'hoped': False, 'clattered': False, 'disused': False, 'chronic': False, 'Egypt': False, 'lurid': False}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# separate training and testing data\n",
    "shuffle(all_data)\n",
    "train_len = math.ceil(len(all_data)*.8)\n",
    "train_data = all_data[:train_len]\n",
    "test_data = all_data[train_len:]\n",
    "test_data_stripped = list(test[0] for test in test_data)\n",
    "print(test_data_stripped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn by NaiveBayes\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    West = True              HPL : EAP    =     83.8 : 1.0\n",
      "                 Raymond = True              MWS : HPL    =     77.4 : 1.0\n",
      "                  misery = True              MWS : EAP    =     71.3 : 1.0\n",
      "                     Old = True              HPL : EAP    =     41.2 : 1.0\n",
      "                 outside = True              HPL : MWS    =     40.8 : 1.0\n",
      "                 curious = True              HPL : EAP    =     40.2 : 1.0\n",
      "             endeavoured = True              MWS : EAP    =     38.9 : 1.0\n",
      "             countenance = True              MWS : HPL    =     37.3 : 1.0\n",
      "                sinister = True              HPL : EAP    =     34.6 : 1.0\n",
      "                     Her = True              MWS : HPL    =     34.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPL\n",
      "HPL\n",
      "0.48071519795657724\n"
     ]
    }
   ],
   "source": [
    "preds = [classifier.classify(test) for test in test_data_stripped]\n",
    "print(preds[0])\n",
    "print(test_data[0][1])\n",
    "\n",
    "accuracy = 0.0\n",
    "len_preds = len(preds)\n",
    "for i in range(len_preds):\n",
    "    accuracy += (preds[i] == test_data[i][1])\n",
    "    \n",
    "accuracy /= len(preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HPL', 'MWS', 'EAP']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8392, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the test data\n",
    "dftest = pd.read_csv(os.path.join(TEXT_DATA_DIR, 'test.csv'))\n",
    "print(dftest.shape)\n",
    "dftest.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
