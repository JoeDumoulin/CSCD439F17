{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a text generation example using RNN units with LSTM.  the example is from [this github page](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "total chars: 57\n",
      "nb sequences: 200285\n",
      "Vectorization...\n",
      "Build model...\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/10\n",
      "200285/200285 [==============================] - 373s 2ms/step - loss: 1.9989\n",
      "Epoch 2/10\n",
      "200285/200285 [==============================] - 380s 2ms/step - loss: 1.6390\n",
      "Epoch 3/10\n",
      "200285/200285 [==============================] - 385s 2ms/step - loss: 1.5469\n",
      "Epoch 4/10\n",
      "200285/200285 [==============================] - 376s 2ms/step - loss: 1.4982\n",
      "Epoch 5/10\n",
      "200285/200285 [==============================] - 338s 2ms/step - loss: 1.4662\n",
      "Epoch 6/10\n",
      "200285/200285 [==============================] - 349s 2ms/step - loss: 1.4473\n",
      "Epoch 7/10\n",
      "200285/200285 [==============================] - 378s 2ms/step - loss: 1.4295\n",
      "Epoch 8/10\n",
      "200285/200285 [==============================] - 375s 2ms/step - loss: 1.4165\n",
      "Epoch 9/10\n",
      "200285/200285 [==============================] - 374s 2ms/step - loss: 1.4060\n",
      "Epoch 10/10\n",
      "200285/200285 [==============================] - 377s 2ms/step - loss: 1.3977\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"along with other means. among men, as am\"\n",
      "along with other means. among men, as among the self-deliver of the world, the sense of the senses and the senses and the subject of the spirit of the serious of the stand the soul and the probably the senses of the more the conscious in the subject of the serious of the case of the soul and self-conception of the senses and the subject of the self-danger of the senses and the subject of the confective in the substitution of the senses \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"along with other means. among men, as am\"\n",
      "along with other means. among men, as among the wisent when the last the very respect the self-conciption of the subject of the postuus and common that the stand the morality and people and the higher and not of the serious and the substance, the\n",
      "actions than in all the casp and individual things of morality, and the \"its of to the world, and contempt of this complece, as\n",
      "a man of the common others\" with the right of his own probably in\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"along with other means. among men, as am\"\n",
      "along with other means. among men, as among also it need of saved, to apident and divined as their centable. in aviet, rifitityuat here for moze of the modern our existence. what that, remotives heselk\n",
      "the new\n",
      "delust who haveds at that\n",
      "agution and\n",
      "applicable that her not and theiends, only does\n",
      "not obligation? has be prohorily will becomes, a drougations, for with the standsual socian of that\n",
      "religion in when in\n",
      "the suchly be one loved\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"along with other means. among men, as am\"\n",
      "along with other means. among men, as among approficudes here rath is sustimentual closel, aver and just early,\n",
      "grietess mistentt. but almost gangis of etaining; is they belief they con\n",
      "possessed that another, which from the hates\n",
      "and ill partimine and meta- as portrical as the conxeduly in oming this staties and bow we casu led to\n",
      "people sunricest se i for thatity. can begines of \"havingus is superfaciculance themaimed to bottrnis meye\n"
     ]
    }
   ],
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 2):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=10)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
